# FILE: backend/scripts/ingest_laws.py
# PHOENIX PROTOCOL - INGESTION SCRIPT V4.1 (ADDED CHUNK INDEX)
# 1. ADDED: chunk_index and total_article_chunks to metadata for proper sorting.
# 2. RETAINED: All previous improvements.
# 3. NOTE: After this change, laws must be re‚Äëingested with --force to update metadata.

import os
import sys
import glob
import hashlib
import argparse
import re
import uuid
from typing import List, Tuple
from pathlib import Path

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    import chromadb
    from app.core.embeddings import JuristiRemoteEmbeddings
    from app.services.text_extraction_service import extract_text
    from app.services.embedding_service import generate_embedding
    from app.services.vector_store_service import get_global_collection, _sanitize_metadata
except ImportError as e:
    print(f"‚ùå MISSING LIBRARIES: {e}")
    print("Run: pip install langchain-community langchain-text-splitters chromadb")
    sys.exit(1)

# --- CONFIGURATION ---
CHROMA_HOST = os.getenv("CHROMA_HOST", "chroma")
CHROMA_PORT = int(os.getenv("CHROMA_PORT", 8000))
COLLECTION_NAME = "legal_knowledge_base"
TARGET_JURISDICTION = 'ks'

print(f"‚öôÔ∏è  CONFIG: Chroma={CHROMA_HOST}:{CHROMA_PORT}")

# ----------------------------------------------------------------------
# NORMALIZATION FUNCTIONS
# ----------------------------------------------------------------------

def clean_text(text: str) -> str:
    """Remove common PDF artifacts."""
    text = re.sub(r'(?m)^={5,}\s*Page\s+\d+\s*={5,}\s*$', '', text, flags=re.IGNORECASE)
    page_patterns = [
        r'(?m)^\s*(?:Faqja|Page|F\.?)\s*\d+\s*(?:/\s*\d+)?\s*$',
        r'(?m)^\s*\d+\s*$',
        r'(?m)^\s*-\s*\d+\s*-\s*$',
        r'(?m)^\s*\[\s*\d+\s*\]\s*$',
    ]
    for pat in page_patterns:
        text = re.sub(pat, '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?m)^.*GAZETA.*ZYRTARE.*$', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?m)^\s*[A-Z\s]*PRISHTIN[√ãE]?\s*$', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?m)^[^\w\s]{10,}$', '', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text.strip()

def extract_law_title(text: str, filename: str) -> str:
    """Extract full law title from document start."""
    sample = text[:5000]

    # LIGJI with number and full description
    match = re.search(r'(LIGJI\s+(?:[Nn]r\.?\s*[\d/]+)\s+[^\n.]+)', sample, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    # LIGJI without number
    match = re.search(r'(LIGJI\s+P√ãR\s+[^\n.]+)', sample, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    # KODI
    match = re.search(r'(KODI\s+(?:[Nn]r\.?\s*[\d/]+)?\s*[^\n.]+)', sample, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    # KUSHTETUTA
    match = re.search(r'(KUSHTETUTA\s+E\s+REPUBLIK√ãS\s+S√ã\s+KOSOV√ãS)', sample, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    # GAZETA ZYRTARE
    match = re.search(r'(GAZETA\s+ZYRTARE[^.\n]*)', sample, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    # Any all-caps line with law keyword
    lines = sample.split('\n')
    for line in lines[:30]:
        if line.isupper() and ('LIGJ' in line or 'KOD' in line or 'KUSHTETUTA' in line):
            return line.strip()
    # Fallback: clean filename
    name = os.path.splitext(filename)[0]
    name = re.sub(r'[_-]', ' ', name)
    name = ' '.join(name.split())
    return name

def split_by_article(text: str) -> List[Tuple[str, str]]:
    """Split into articles based on 'Neni' or 'Art.' markers."""
    lines = text.split('\n')
    article_starts = []
    for i, line in enumerate(lines):
        if re.match(r'^\s*(?:Neni|Art\.?)\s+[\d\.]+', line, re.IGNORECASE):
            article_starts.append(i)
    if not article_starts:
        return [("1", text.strip())]
    articles = []
    for idx, start_idx in enumerate(article_starts):
        line = lines[start_idx]
        match = re.search(r'(?:Neni|Art\.?)\s+([\d\.]+)', line, re.IGNORECASE)
        article_num = match.group(1) if match else "0"
        end_idx = article_starts[idx + 1] if idx + 1 < len(article_starts) else len(lines)
        content = '\n'.join(lines[start_idx:end_idx]).strip()
        articles.append((article_num, content))
    return articles

def calculate_file_hash(filepath: str) -> str:
    hasher = hashlib.md5()
    try:
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not hash file {filepath}: {e}")
        return ""

# ----------------------------------------------------------------------
# MAIN INGESTION
# ----------------------------------------------------------------------

def ingest_legal_docs(directory_path: str, force_reingest: bool = False, chunk_size: int = 1000):
    abs_path = os.path.abspath(directory_path)
    print(f"üìÇ Scanning Directory: {abs_path}")

    if not os.path.isdir(directory_path):
        print(f"‚ùå Directory not found: {directory_path}")
        return

    print(f"üîå Connecting to ChromaDB...")
    try:
        client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=JuristiRemoteEmbeddings()
        )
        print("‚úÖ Connected.")
    except Exception as e:
        print(f"‚ùå DB Connection Failed: {e}")
        return

    supported_extensions = ['*.pdf', '*.PDF']
    all_files = []
    for ext in supported_extensions:
        found = glob.glob(os.path.join(directory_path, "**", ext), recursive=True)
        all_files.extend(found)
    all_files = sorted(set(all_files))

    if not all_files:
        print(f"‚ö†Ô∏è No PDF files found in {directory_path}")
        return

    print(f"üìö Found {len(all_files)} files. Starting...")

    stats = {"skipped": 0, "added": 0, "updated": 0, "failed": 0}

    for file_path in all_files:
        filename = os.path.basename(file_path)
        print(f"\n--- Processing: {filename} ---")

        try:
            current_hash = calculate_file_hash(file_path)

            # Check existing
            if not force_reingest:
                existing = collection.get(where={"source": filename}, limit=1, include=["metadatas"])
                if existing['ids'] and existing['metadatas'] and existing['metadatas'][0].get("file_hash") == current_hash:
                    print(f"‚è≠Ô∏è  Skipped (unchanged)")
                    stats["skipped"] += 1
                    continue

            # Delete old vectors
            collection.delete(where={"source": filename})
            print(f"üóëÔ∏è  Deleted old vectors")

            # Extract text (with OCR fallback)
            print("üìÑ Extracting text...")
            full_text = extract_text(file_path, "application/pdf")
            if not full_text or len(full_text.strip()) < 50:
                print("‚ö†Ô∏è  Extracted text too short, may be scanned or empty.")
                # The file may need OCR. Since pytesseract is installed, it should work.
                stats["failed"] += 1
                continue

            # Clean
            full_text = clean_text(full_text)
            if not full_text:
                print("‚ö†Ô∏è  Text became empty after cleaning.")
                stats["failed"] += 1
                continue

            # Extract title
            law_title = extract_law_title(full_text, filename)
            print(f"üè∑Ô∏è  Title: {law_title}")

            # Split articles
            articles = split_by_article(full_text)
            print(f"üìñ Detected {len(articles)} article(s)")

            # Prepare batches
            batch_ids = []
            batch_texts = []
            batch_metadatas = []
            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))

            for article_num, article_content in articles:
                chunks = splitter.split_text(article_content)
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{filename}_{current_hash[:8]}_art{article_num}_ch{i}_{uuid.uuid4()}"
                    batch_ids.append(chunk_id)
                    batch_texts.append(chunk)
                    meta = {
                        "source": filename,
                        "law_title": law_title,
                        "article_number": str(article_num),
                        "type": "LAW",
                        "jurisdiction": TARGET_JURISDICTION,
                        "file_hash": current_hash,
                        "page": 0,
                        "chunk_index": i,                       # NEW: index within article
                        "total_article_chunks": len(chunks)     # NEW: total chunks for this article
                    }
                    # Sanitize metadata (convert non‚Äëscalars)
                    meta = {k: (v if v is not None else "") for k, v in meta.items()}
                    batch_metadatas.append(meta)

            # Add to ChromaDB in batches
            BATCH_SIZE = 50
            for i in range(0, len(batch_ids), BATCH_SIZE):
                collection.add(
                    ids=batch_ids[i:i+BATCH_SIZE],
                    documents=batch_texts[i:i+BATCH_SIZE],
                    metadatas=batch_metadatas[i:i+BATCH_SIZE]
                )
                print(".", end="", flush=True)
            print(" ‚úÖ")
            stats["added"] += 1

        except Exception as e:
            print(f"‚ùå Error: {e}")
            stats["failed"] += 1

    print("\n" + "-"*50)
    print(f"üèÅ Ingestion Complete.")
    print(f"   Added:   {stats['added']}")
    print(f"   Updated: {stats['updated']}")
    print(f"   Skipped: {stats['skipped']}")
    print(f"   Failed:  {stats['failed']}")
    print("-"*50)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("path", nargs="?", default="/app/data/laws", help="Path to documents folder")
    parser.add_argument("--force", action="store_true", help="Force re‚Äëingest")
    parser.add_argument("--chunk-size", type=int, default=1000)
    args = parser.parse_args()
    ingest_legal_docs(args.path, force_reingest=args.force, chunk_size=args.chunk_size)