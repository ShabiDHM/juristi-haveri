# FILE: backend/app/services/document_processing_service.py
# PHOENIX PROTOCOL - ACCOUNTING HYDRA ORCHESTRATOR V16.0
# 1. REFACTOR: Transformed progress messages to "Klient/Biznes" terminology.
# 2. FIX: Updated placeholder validation to detect accounting-related patterns.
# 3. COMPATIBILITY: Maps 'document_number' or 'fiscal_number' to legacy 'case_number' field.
# 4. STATUS: 100% Accounting Aligned.

import os
import tempfile
import logging
import shutil
import json
import asyncio
import hashlib
from typing import List, Dict, Any, cast, Optional, Tuple
from datetime import datetime, timezone

from pymongo.database import Database
import redis
from bson import ObjectId

# Phoenix: Enforcing Absolute Imports
from app.services import (
    document_service, 
    storage_service, 
    llm_service, 
    text_extraction_service, 
    conversion_service,
    deadline_service
)
from app.services.graph_service import graph_service 
from app.services.categorization_service import CATEGORIZATION_SERVICE
from app.services.albanian_language_detector import AlbanianLanguageDetector
from app.services.albanian_document_processor import EnhancedDocumentProcessor
from app.services.albanian_metadata_extractor import albanian_metadata_extractor
from app.models.document import DocumentStatus

# Absolute imports for vector store functions
from app.services.vector_store_service import (
    delete_document_embeddings,
    create_and_store_embeddings_from_chunks,
    copy_document_embeddings
)

logger = logging.getLogger(__name__)

OCR_FALLBACK_THRESHOLD = 100

class DocumentNotFoundInDBError(Exception):
    pass

def _compute_file_hash(file_path: str) -> str:
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(65536), b''):
            sha256.update(chunk)
    return sha256.hexdigest()

def _stringify_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    result = {}
    for k, v in metadata.items():
        if v is None:
            result[k] = None
        elif isinstance(v, (str, int, float, bool)):
            result[k] = v
        elif isinstance(v, (dict, list)):
            try:
                result[k] = json.dumps(v, ensure_ascii=False)
            except Exception:
                result[k] = str(v)
        else:
            result[k] = str(v)
    return result

async def _emit_progress_async(redis_client: redis.Redis, user_id: str, doc_id: str, message: str, percent: int):
    try:
        if not user_id or not redis_client: return
        channel = f"user:{user_id}:updates"
        payload = {"type": "DOCUMENT_PROGRESS", "document_id": doc_id, "message": message, "percent": percent}
        await asyncio.to_thread(redis_client.publish, channel, json.dumps(payload))
    except Exception: pass

def _is_placeholder_text(text: str) -> bool:
    """
    Returns True if the text appears to be a system-generated placeholder.
    Updated for accounting context.
    """
    if ("Biznesi:" in text or "Klienti:" in text) and "| Kontabilisti AI System" in text:
        return True
    if len(text.strip()) < 50:
        return True
    return False

async def orchestrate_document_processing_mongo(db: Database, redis_client: redis.Redis, document_id_str: str):
    try:
        doc_id = ObjectId(document_id_str)
    except Exception: return

    document = await asyncio.to_thread(db.documents.find_one, {"_id": doc_id})
    if not document: raise DocumentNotFoundInDBError(document_id_str)

    user_id = str(document.get("owner_id"))
    doc_name = document.get("file_name", "Unknown Document")
    case_id_str = str(document.get("case_id"))
    
    await _emit_progress_async(redis_client, user_id, document_id_str, "Inicializimi...", 5)

    temp_original_file_path = ""
    file_hash = None
    
    try:
        suffix = os.path.splitext(doc_name)[1]
        temp_file_descriptor, temp_original_file_path = tempfile.mkstemp(suffix=suffix)
        os.close(temp_file_descriptor) 
        
        file_stream = await asyncio.to_thread(storage_service.download_original_document_stream, document["storage_key"])
        with open(temp_original_file_path, 'wb') as temp_file:
            await asyncio.to_thread(shutil.copyfileobj, file_stream, temp_file)
        if hasattr(file_stream, 'close'): file_stream.close()

        file_hash = _compute_file_hash(temp_original_file_path)

        # --- Deduplication ---
        existing_doc = await asyncio.to_thread(
            db.documents.find_one,
            {
                "file_hash": file_hash,
                "case_id": document["case_id"],
                "owner_id": document["owner_id"],
                "processed_text_key": {"$exists": True, "$ne": None},
                "status": {"$ne": DocumentStatus.FAILED}
            }
        )
        
        if existing_doc:
            logger.info(f"Duplicate document detected (hash: {file_hash}). Copying metadata...")
            await _emit_progress_async(redis_client, user_id, document_id_str, "Dokument i dyfishuar – kopjohet...", 50)
            
            text_key = existing_doc.get("processed_text_key")
            preview_key = existing_doc.get("preview_key")
            
            copy_success = False
            try:
                await asyncio.to_thread(
                    copy_document_embeddings,
                    source_document_id=str(existing_doc["_id"]),
                    target_document_id=document_id_str,
                    target_user_id=user_id,
                    target_case_id=case_id_str
                )
                copy_success = True
            except Exception as e:
                logger.warning(f"copy_document_embeddings failed – will reprocess. Error: {e}")
            
            if copy_success:
                update_data = {
                    "status": "PROCESSED",
                    "processed_text_key": text_key,
                    "preview_key": preview_key,
                    "file_hash": file_hash,
                    "detected_language": existing_doc.get("detected_language"),
                    "category": existing_doc.get("category"),
                    "metadata": existing_doc.get("metadata", {}),
                    "summary": existing_doc.get("summary"),
                    "processing_time": datetime.now(timezone.utc)
                }
                
                await asyncio.to_thread(db.documents.update_one, {"_id": doc_id}, {"$set": update_data})
                await _emit_progress_async(redis_client, user_id, document_id_str, "Përfunduar (kopjuar)", 100)
                return

        # --- Text Extraction with OCR Fallback ---
        await _emit_progress_async(redis_client, user_id, document_id_str, "Ekstraktimi i të dhënave...", 20)
        
        raw_text = await asyncio.to_thread(
            text_extraction_service.extract_text, 
            temp_original_file_path, 
            document.get("mime_type", "")
        )
        
        if (not raw_text or len(raw_text.strip()) < OCR_FALLBACK_THRESHOLD):
            ocr_fn = getattr(text_extraction_service, "extract_text_with_ocr", None)
            if ocr_fn:
                logger.warning(f"Low content ({len(raw_text or '')} chars). Attempting OCR...")
                await _emit_progress_async(redis_client, user_id, document_id_str, "OCR në progres...", 25)
                raw_text = await asyncio.to_thread(ocr_fn, temp_original_file_path, document.get("mime_type", ""))
            
        if not raw_text or not raw_text.strip():
            raise ValueError("Teksti i ekstraktuar është bosh.")

        if _is_placeholder_text(raw_text):
            raise ValueError("Ekstraktimi i tekstit dështoi – u mor tekst i pavlefshëm.")

        # --- Metadata & Categorization ---
        await _emit_progress_async(redis_client, user_id, document_id_str, "Analiza e strukturës...", 35)
        
        sterilized_text = llm_service.sterilize_legal_text(raw_text)
        is_albanian = AlbanianLanguageDetector.detect_language(sterilized_text)
        
        extracted_metadata = await asyncio.to_thread(
            albanian_metadata_extractor.extract, 
            sterilized_text, 
            document_id_str
        )
        
        # PHOENIX: Map Accounting keys back to Infrastructure fields
        # If extractor found a document number or fiscal number, use it as the case_number
        if "case_number" not in extracted_metadata or not extracted_metadata["case_number"]:
            extracted_metadata["case_number"] = extracted_metadata.get("document_number") or extracted_metadata.get("fiscal_number")

        extracted_metadata = _stringify_metadata(extracted_metadata)
        
        try:
            detected_category = await asyncio.to_thread(CATEGORIZATION_SERVICE.categorize_document, sterilized_text)
        except Exception:
            detected_category = "Të tjera"
        
        await asyncio.to_thread(
            db.documents.update_one, 
            {"_id": doc_id}, 
            {"$set": {
                "detected_language": "sq" if is_albanian else "en", 
                "category": detected_category, 
                "metadata": extracted_metadata,
                "file_hash": file_hash
            }}
        )

        # --- Async Summary Generation ---
        await _emit_progress_async(redis_client, user_id, document_id_str, "Gjenerimi i analizës AI...", 50)
        summary_task = llm_service.process_large_document_async(sterilized_text)

        # --- Parallel Tasks ---
        async def task_embeddings():
            await asyncio.to_thread(delete_document_embeddings, user_id=user_id, document_id=document_id_str)
            
            enriched_chunks = await asyncio.to_thread(
                EnhancedDocumentProcessor.process_document, 
                text_content=raw_text,
                document_metadata={
                    'category': detected_category, 
                    'file_name': doc_name,
                    **extracted_metadata
                }, 
                is_albanian=is_albanian
            )
            
            for chunk in enriched_chunks:
                if 'page' not in chunk.metadata or not chunk.metadata['page']:
                    chunk.metadata['page'] = 1
                chunk.metadata['document_type'] = extracted_metadata.get('document_type', detected_category)
                chunk.metadata['case_id'] = case_id_str
                chunk.metadata['owner_id'] = user_id
                chunk.metadata['language'] = 'sq' if is_albanian else 'en'
                chunk.metadata['file_hash'] = file_hash
                if 'case_number' in chunk.metadata:
                    chunk.metadata['case_number'] = str(chunk.metadata['case_number'])
            
            success = await asyncio.to_thread(
                create_and_store_embeddings_from_chunks,
                user_id=user_id, 
                document_id=document_id_str, 
                case_id=case_id_str, 
                file_name=doc_name, 
                chunks=[c.content for c in enriched_chunks], 
                metadatas=[c.metadata for c in enriched_chunks]
            )
            if not success:
                raise RuntimeError("Embedding creation failed.")
            return enriched_chunks

        async def task_storage():
            return await asyncio.to_thread(
                storage_service.upload_processed_text, 
                raw_text, user_id, case_id_str, document_id_str
            )

        async def task_deadlines():
            # Triggers Fiscal Deadline Engine
            await asyncio.to_thread(
                deadline_service.extract_and_save_deadlines, 
                db, document_id_str, sterilized_text, detected_category
            )

        async def task_graph():
            graph_data = await asyncio.to_thread(llm_service.extract_graph_data, sterilized_text)
            entities = graph_data.get("nodes") or graph_data.get("entities") or []
            relations = graph_data.get("edges") or graph_data.get("relations") or []
            await asyncio.to_thread(
                graph_service.ingest_entities_and_relations, 
                case_id=case_id_str, 
                document_id=document_id_str, 
                doc_name=doc_name, 
                entities=entities, 
                relations=relations, 
                doc_metadata=extracted_metadata
            )

        async def task_preview():
            pdf_path = await asyncio.to_thread(conversion_service.convert_to_pdf, temp_original_file_path)
            key = await asyncio.to_thread(
                storage_service.upload_document_preview, 
                pdf_path, user_id, case_id_str, document_id_str
            )
            return pdf_path, key

        await _emit_progress_async(redis_client, user_id, document_id_str, "Përpunimi i inteligjencës...", 75)
        
        results = await asyncio.gather(
            summary_task,
            task_embeddings(),
            task_storage(),
            task_deadlines(),
            task_graph(),
            task_preview(),
            return_exceptions=True
        )
        
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                raise RuntimeError(f"Task {i} failed: {res}")
        
        final_summary = cast(str, results[0])
        text_key = cast(str, results[2])
        preview_result = cast(Tuple[str, str], results[5])
        pdf_temp_path, preview_storage_key = preview_result
        
        await _emit_progress_async(redis_client, user_id, document_id_str, "Përfunduar", 100)
        await asyncio.to_thread(
            document_service.finalize_document_processing, 
            db, redis_client, document_id_str, final_summary, text_key, preview_storage_key
        )

        if pdf_temp_path and os.path.exists(pdf_temp_path):
            os.remove(pdf_temp_path)

    except Exception as e:
        logger.error(f"Dështim gjatë procesimit: {e}")
        try:
            await asyncio.to_thread(delete_document_embeddings, user_id=user_id, document_id=document_id_str)
        except: pass
        
        await asyncio.to_thread(
            db.documents.update_one, 
            {"_id": doc_id}, 
            {"$set": {
                "status": DocumentStatus.FAILED, 
                "error_message": str(e),
                "file_hash": file_hash
            }}
        )
        raise e
    finally:
        if temp_original_file_path and os.path.exists(temp_original_file_path): 
            os.remove(temp_original_file_path)